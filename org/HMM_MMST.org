# -*- coding: utf-8 -*-
# -*- mode: org -*-
#+startup: beamer
#+STARTUP: overview
#+STARTUP: indent
#+TAGS: noexport(n)
#+LANGUAGE: en

#+Title:  Hidden Markov Model implementation for Mixture of Multiple Scale T-distributions
#+AUTHOR:      Steven QUINITO MASNADA

#+EPRESENT_FRAME_LEVEL: 2

#+LaTeX_CLASS: beamer
#+LaTeX_CLASS_OPTIONS: [11pt,xcolor=dvipsnames,presentation]
#+OPTIONS:   H:3 num:t toc:nil \n:nil @:t ::t |:t ^:nil -:t f:t *:t <:t

#+LATEX_HEADER: \usedescriptionitemofwidthas{bl}
#+LATEX_HEADER: \usepackage[T1]{fontenc}
#+LATEX_HEADER: \usepackage[utf8]{inputenc}
#+LATEX_HEADER: \usepackage[american]{babel}
#+LATEX_HEADER: \usepackage{amsmath,amssymb,amsthm,amsfonts}
#+LATEX_HEADER: \usepackage{bbm}
#+LATEX_HEADER: \usepackage{boxedminipage,xspace,multicol}
#+LATEX_HEADER: %%%%%%%%% Begin of Beamer Layout %%%%%%%%%%%%%
#+LATEX_HEADER: \ProcessOptionsBeamer
#+latex_header: \mode<beamer>{\usetheme{Madrid}}
#+LATEX_HEADER: \usecolortheme{whale}
#+LATEX_HEADER: \usecolortheme[named=BrickRed]{structure}
# #+LATEX_HEADER: \useinnertheme{rounded}
#+LATEX_HEADER: \useoutertheme{infolines}
#+LATEX_HEADER: \setbeamertemplate{footline}[frame number]
#+LATEX_HEADER: \setbeamertemplate{headline}[default]
#+LATEX_HEADER: \setbeamertemplate{navigation symbols}{}
#+LATEX_HEADER: \defbeamertemplate*{headline}{info theme}{}
#+LATEX_HEADER: \defbeamertemplate*{footline}{info theme}{\leavevmode%
#+LATEX_HEADER:   \hbox{%
#+LATEX_HEADER:     \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
#+LATEX_HEADER:       \usebeamerfont{author in head/foot}\insertshortauthor
#+LATEX_HEADER:     \end{beamercolorbox}%
#+LATEX_HEADER:   \begin{beamercolorbox}[wd=.41\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
#+LATEX_HEADER:     \usebeamerfont{title in head/foot}\insertsectionhead
#+LATEX_HEADER:   \end{beamercolorbox}%
#+LATEX_HEADER:   \begin{beamercolorbox}[wd=.09\paperwidth,ht=2.25ex,dp=1ex,right]{section in head/foot}%
#+LATEX_HEADER:     \usebeamerfont{section in head/foot}\insertframenumber{}~/~\inserttotalframenumber\hspace*{2ex} 
#+LATEX_HEADER:   \end{beamercolorbox}
#+LATEX_HEADER:   }\vskip0pt}
#+LATEX_HEADER: \setbeamertemplate{footline}[info theme]
#+LATEX_HEADER: %%%%%%%%% End of Beamer Layout %%%%%%%%%%%%%
#+LATEX_HEADER: \usepackage{verbments}
#+LATEX_HEADER: \usepackage{xcolor}
#+LATEX_HEADER: \usepackage{color}
#+LATEX_HEADER: \usepackage{url} \urlstyle{sf}
#+LATEX_HEADER: \usepackage{appendixnumberbeamer}
#+LATEX_HEADER: \usepackage{multicol}

#+LATEX_HEADER: \let\alert=\structure % to make sure the org * * works of tools
#+BEAMER_FRAME_LEVEL: 2

#+LATEX_HEADER: \AtBeginSection[]{\begin{frame}<beamer>\frametitle{Talk Outline}\tableofcontents[currentsection]\end{frame}}

#+LATEX_HEADER: %\usepackage{biblatex}
# #+LATEX_HEADER: \bibliography{../../biblio.bib}
# #+LATEX_HEADER: \usepackage{cite}

#+LATEX_HEADER: \usepackage{xparse}

# Custom Commands
#+LATEX_HEADER: \DeclareMathOperator*{\argmax}{arg\,max}
#+LATEX_HEADER: \DeclareMathOperator*{\argmin}{arg\,min}

#+LATEX_HEADER: \newcommand{\step}[1][]{^{(#1)}}
#+LATEX_HEADER: \newcommand{\eigenv}[2]{D_{#1}\ifthenelse{\equal{#2}{}}{^{(#2)}}{}}
#+LATEX_HEADER: \DeclareDocumentCommand{\talpha}{ o o }{\tilde{\alpha} \IfValueT{#1}{_{#1}} \IfValueT{#2}{^{(#2)}}}
#+LATEX_HEADER: \DeclareDocumentCommand{\tdelta}{ o o }{\tilde{\delta} \IfValueT{#1}{_{#1}} \IfValueT{#2}{^{(#2)}}}
#+LATEX_HEADER: \DeclareDocumentCommand{\tDelta}{ o o }{\tilde{\Delta} \IfValueT{#1}{_{#1}} \IfValueT{#2}{^{(#2)}}}
#+LATEX_HEADER: \DeclareDocumentCommand{\tgamma}{ o o }{\tilde{\gamma} \IfValueT{#1}{_{#1}} \IfValueT{#2}{^{(#2)}}}
#+LATEX_HEADER: \DeclareDocumentCommand{\A}{ o o }{ A \IfValueT{#1}{_{#1}} \IfValueT{#2}{^{(#2)}}}
#+LATEX_HEADER: \DeclareDocumentCommand{\D}{ o o }{ D \IfValueT{#1}{_{#1}} \IfValueT{#2}{^{(#2)}}}

#+BEGIN_LaTeX
\newcommand{\backupbegin}{
   \newcounter{finalframe}
   \setcounter{finalframe}{\value{framenumber}}
}
\newcommand{\backupend}{
   \setcounter{framenumber}{\value{finalframe}}
}
#+END_LaTeX

#+BEGIN_LaTeX
\setbeamertemplate{caption}{\raggedright\insertcaption\par}
#+END_LaTeX

* Objective of this document
*** Objective of this document
- *Gather information* to implement HMRF based clustering
- Documentation of implementation \to *translation* from the formula to the code
  (pseudo code)
- Explanation of *optimization strategies* (memory access pattern, GPUs,
  etc...)\to maybe a tutorial
* Glossary
*** DONE Symbol definitions
#+LaTeX: \scriptsize
| Definition                                                                                                         |
|--------------------------------------------------------------------------------------------------------------------|
| $N \in \mathbb{N_{+}}$: Number of observations                                                                           |
| $K \in \mathbb{N}_{+}^{*}$: Number of clusters                                                                         |
| $M \in \mathbb{N}_{+}^{*}$: Number of Observation parameters (AUC, ADC, T1map, etc...), Gaussian dimension of a MST   |
|--------------------------------------------------------------------------------------------------------------------|
| $Y = {Y_{1},...,Y_{N}}, Y_{n} \in \mathbb{R}^{M}$: observed variables                                                            |
| $Z = {Z_{1},...,Z_{N}}, Z_{n} \in \{0,1\}^{K}$: HMRF for the clusters (Gibbs distribution)                                       |
| $W = {W_{1},...,W_{N}}, W_{n} \in (\mathbb{R}_{+}^{*})^{M}$: latent variable to quantify the observation information                   |
|--------------------------------------------------------------------------------------------------------------------|
| $\mu \in \mathbb{R}^{M \times K}$: mean (one mean per gaussian component of MST?)                                              |
| $D \in \mathbb{R}^{M \times M \times K}$: eigenvectors                                                                      |
| $A \in \mathbb{R}^{M \times M \times K}$: eigenvalues (diagonal matrix)                                                      |
| $\Sigma \in \mathbb{R}^{M \times M \times K}$: covariance matrix = $DAD^{T}$                                                         |
| $\gamma \in \mathbb{R_{+}^{*}}^{M \times K}$: degree of freedom shape of the Gamma Laws of the MST                                      |
| $\delta \in \mathbb{R}^{M \times K}$: degree of freedom intensity of the Gamma Law MST                                            |
| $\Theta = (\mu, \Sigma, \gamma, \delta)$: clusters parameters                                                                            |
| $\alpha \in \mathbb{R}^{K}$: external field parameters of Z                                                                  |
| $\beta \in \mathbb{R}$: interaction parameters of Z                                                                      |
| $\psi = (\alpha,\beta)$: parameters of Z                                                                                       |
| $\talpha  \in \mathbb{R}^{N \times K}$: parameters of the posterior distribution                                             |
| $\tdelta \in \mathbb{R}^{M \times K \times N}$: parameters of the posterior distribution                                          |
| $\tgamma \in \mathbb{R}^{K \times N}$: parameters of the posterior distribution                                                   |
| $\Delta_{n} \in (\mathbb{R}_{+}^{*})^{M \times M} = diag(w_{n})$                                                                              |
|--------------------------------------------------------------------------------------------------------------------|
| S: Sites                                                                                                           |
| \Omega(n): Neighbors of n with n \in S                                                                                    |
#+LaTeX: \normalsize  
* TODO Hidden Markov Models with MMST
** DONE General
*** Random Variables
**** TODO Y - Observation as a MMST
- Mixture of MST:
$$MMST(y;\pi,\theta) = \sum_{k=1}^{K} \pi_{k} MST(y;\theta_{k})$$

# TODO: 
# - The formula is not correct 
# - maybe it is not useful have this here. 
# - The dimensionality of the gaussian is not correct
- A infinite mixture of Gaussians:
$$MST(y;\mu,\Sigma,\delta,\gamma) = \int_{0}^{\infty} \mathcal{N}_{M}(y;\mu,D\Delta_{w}^{-1}AD^{T})
\mathcal{G}(w,\delta,\gamma)dw$$
**** Z - Labels as an HMRF
$$Z \sim P_{G}(\alpha,\beta)$$
**** W - Weights as a latent variable
$$W | Z \sim \mathcal{G}(\delta,\gamma)$$
*** HMRF                                                         :noexport:
# Not sure it is really necessary here, we don't care because formulas
# are already given.
**** Distribution of Z - Gibbs distribution
#+BEGIN_LaTeX
\begin{equation}
p(z;\psi) = K(\psi)^{-1} exp [H(z;\psi)]
\end{equation}
#+END_LaTeX
- Energy function
  \begin{equation}
  H(z; \psi) = \sum_{n=1}^{N} \bigg[ \alpha_{z_n} + \frac{\beta}{2} \sum_{l\in \Omega (n) } \mathbbm{1} (z_n = z_l) \bigg]
  \end{equation}

- With partition function:
  \begin{equation}
  K(\psi) = \sum_{z} \exp [H(z; \psi)]
  \end{equation}

$\alpha_{z_{n}}$ : weight of the class to which $z_{n}$ belongs\\
$\mathbbm{1}(z_n = z_l)$ : Kronecker index \to return 1 if $n$ and $l$
have the same label.

*** EM for HMM
**** E-step
- Expected complete likelihood \to approximate the posterior $p(w,z|y)$ with $q_{w,z}(w,z)$
**** M-step
1. update the mean
2. update the eigenvectors
3. update the eigenvalues
4. update the degree of freedom
5. update energy parameters
** DONE E Step
*** DONE E-step
- Complete likelihood:
  $$p(y,w,z|\theta) = p(w,z|\psi,\delta,\gamma) p(y|w,z,\theta)$$

- Posterior approximation:
  - W, Z
  - W | Z
  - Z

However Z cannot be computed \to approximated using variational methods
(mean field approximation).

Thus the posterior is computed at each site.
*** E step                                                       :noexport:
# Do details to explain the e-step, maybe more general explanation
- compute the expected complete log-likelihood:
  - Posterior probabilities:
    - In the case of HMM, posterior probabilities cannot be computed
      and must be approximated. Thus p(z,w|y) is approximated with the
      distribution q(z,w). Here q is factorized as independent
      distributions and is computed at each site.
      # Where q is a multinomial law 
      #+BEGIN_LaTeX
      \begin{equation}
      q(w,z) = \prod_{i}q_i(w_i,z_i)
      \end{equation}
      #+END_LaTeX
      with i \in S, w \in W and z \in Z.

      #+BEGIN_LaTeX
      \begin{equation}
      q_i^{t+1}(w_i,z_i) \propto exp \bigg[\mathbbm{E}_q_{i}^{t} \ln p(w_i,z_i|y, W_i^t, Z_i^t; \phi^t)\bigg]
      \end{equation}
      #+END_LaTeX
*** E step - Expected posterior                                  :noexport:
# Not really needed because we are find the posterior with the
# equation further below
- Expected posterior:
  \begin{equation}
  \begin{split}
  \mathbb{E}_{q_{w_{z_{n},w_{n}}}} [\ln p(w_n,z_n|y, w_n^{(r)}, z_n^{(r)};
  \phi^{(r)})] \\
  \approx \sum_{k=1}^{K} \mathbb{I}_{e_{k}}(z_{n}) \sum_{m=1}^{M} [(\tilde{\gamma}_{km} - 1)
  \ln(w_{nm}) - \tilde{\delta}_{knm} w_{nm} + \tilde{\gamma}_{km} \ln(\tilde{\delta}_{knm}) - ln \Gamma(\tilde{\gamma}_{km})] \\
  + \sum_{k=1}^{K} \mathbb(I)_{e_{k}} [ \tilde{\alpha}_{z_{nk}} + \frac{\beta^{(r)}}{2} \sum_{l \in \Omega(n)} q_{z_{l}}^{(r)}(e_{k})]
  \end{split}
  \end{equation}
*** DONE E step - Initialization
$q_{z_{n}}$ can be initialized at random or with an algorithm such as k-means
**** DONE Questions [1/1]
- [X] How are initialized \alpha, \beta, \gamma, \delta ?
  One of the \alpha value can be initialized to 0 and the others to 1/k.
  But if we initialize with k-mean we do not need to initialized the
  above variables and we can start at M-step to update the
  parameters.
*** DONE E step - variables
$\alpha_{z_{n}}$: external field for the corresponding label for the site n

$\delta_{km}$: degree of freedom intensity of the $m^{th}$ normal in the $k^{th}$
cluster.

$\gamma_{km}$: degree of freedom shape of the $m^{th}$ normal in the $k^{th}$
cluster.

$a_{km}$: $m^{th}$ diagonal element of $A_{k}$

$\tilde{\delta}_{knm} = \delta_{km} + \frac{1}{2} [ A_{k}^{(r)} D_{k}^{(r)}^{t} (y_{n} - \mu_{k}^{(r)}) (y_{n} - \mu_{k}^{(r)})^{t}
          D_{k}^{(r)} ]_{mm}$

$\tilde{\gamma}_{km} = \gamma_{km} + \frac{1}{2}$

\[\tilde{\alpha}_{z_{n}k} = \alpha_{z_{n}}^{(r)} + \sum_{m=1}^{M} [ \ln(a_{km}^{(r)}) + \gamma_{km} \ln(\delta_{km}) -
             ln \Gamma(\gamma_{km}) - \tilde{\gamma}_{km} \ln(\tilde{\delta}_{knm}) + \ln \Gamma(\tilde{\gamma}_{km})]\]
*** DONE E step - Update z
**** Equation
\begin{equation}
q_{z_{n}}^{(r+1)}(e_{k}) = \frac{ exp[ \tilde{\alpha}_{z_{nk}} +
      \frac{\beta^{(r)}}{2} \sum_{l \in \Omega(n)} q_{z_{l}}^{(r)} (e_{k})]}
      {\sum_{j=1}^{K} exp[ \tilde{\alpha}_{z_{nj}} +
      \frac{\beta^{(r)}}{2} \sum_{l \in \Omega(n)} q_{z_{l}}^{(r)} (e_{j})]}
\end{equation}
**** DONE Questions [5/5]
- [X] Do we really use the updated z map in side the r iteration?
  Yes, we take into account the updated one and also the not updated one.
- [X] Would it be wrong to not take into account the modification at
  step r for easier parallelism?
  Convergence properties no guarantied.
- [X] Do we use as well the updated version of \alpha?
  No it is just updated at each iteration r.
- [X] Is M the normal dimension of a MST?
  Yes
- [X] What are $\tilde{\alpha}_{z_{nk}}$ $\delta$ and $\gamma$?
  +Update of the \alpha $\tdelta$ and $\tgamma$ for the posterior.+
  They are just a re-writing to expose the gamma distribution.
*** DONE E step - Update w|z
**** Equation
\begin{equation}
q_{w_{n}|z_{n}}^{(r+1)}(w_{n} | z_{n} = e_{k}) = \prod_{m=1}^{M} \tilde{\delta}_{knm}^{\tilde{\gamma}_{km}} \Gamma(\tilde{\gamma}_{km})^{-1} w_{nm}^{(\tilde{\gamma}_{km}-1)} exp(- \tilde{\delta}_{knm} w_{nm})
\end{equation}

**** Remarks
- Here we can spot that this is just a product of gamma distributions
**** DONE Questions [1/1]
- [X] What is $w_{n}$ ? Which values can it take?
  It is the proximity of an observation to the center of the
  class. The closer from the center, the more important. In other
  words it is the quantity of information provided by an observation.
  # Proximité avec le centre de la classe. Plus une observation est
  # proche du centre plus elle à de l'importance. Quantité d'information
  # apporté par une observation.
*** DONE E step - Update w,z
**** Equation
\begin{equation}
q_{w_{n},z_{n}}^{(r+1)}(w_{n},z_{n}) =  \prod_{k=1}^{K} q_{w_{n}|z_{n}}^{(r+1)}(w_{n} | z_{n} = e_{k}) . q_{z_{n}}^{(r+1)}(z_{n}_{})
\end{equation}

**** TODO Questions [1/2]
- [X] $q_{_z_{n}}^{(r+1)} (z_{n} = e_{k})$ ?
  In fact the second term in not in the product as I suspected.
- [ ] Check if we really need to compute w,z.
** TODO M Step
*** DONE M step
- Maximize the expected complete log-likelihood:
  - Step 1:
    - Update the mean
    - Update the eigenvectors
    - Update the eigenvalues
  - Step 2:
    - Update degree of freedom
  - Step 3:
    - Update Gibbs distribution parameters
*** TODO M Step - Variables
# TODO:
# - Add how the formula was found 
$\tilde{\Delta}_{nk}^{(r+1)} = \mathbbm{E}_{q_{w_{n}|z_{n} = e_{k}}}^{(r+1)} (\Delta_{n}) =
\frac{\tgamma[mk][r]}{\tdelta[nkm][r]}$

**** TODO Questions [2/3]
- [ ] How do we compute it? Is it something like:
$\mathbbm{E}_{q_{w_{n}|z_{n} = e_{k}}}^{(r+1)} (\Delta_{n}) = \int_{0}^{\infty}
\Delta_{n} q_{w_{n}|z_{n}}(w_{n},z_{n})dw_{n}$

- [X] On what do we integrate? \Delta?
  We integrate on w_{n}.

- [X] Is it supposed to be $\Delta_{n}^{(r)$} instead of just $\Delta_{n}$?
  No because \Delta_{n} does not change, only the esperance change because of q_{w_{n}|z_{n}}.
*** DONE M Step - update mean \mu
**** Equation
\begin{equation}
\mu_{km}^{(r+1)} = \frac{\sum_{n=1}^{N} q_{z_{n}}^{(r+1)}(e_{k}) \Big[
D_{k}^{(r)} \tDelta[nk][r+1] D_{k}^{(r)}^{t} y_{n }\Big]_{m}}
              {\sum_{n=1}^{N} q_{z_{n}}^{(r+1)}(e_{k}) \tDelta[nkm][r+1]}
\end{equation}
*** TODO M Step - update eigenvectors D
**** Equation
\begin{equation}
D_{k}^{(r+1)} =  \argmin_{D_{k}} \sum_{n=1}^{N} tr \Big[ \D[k] \tDelta[nk][r+1] \A[k][r] \D[k]^{t} (y_{n} - \mu_{k}^{(r+1)}) (y_{n} - \mu_{k}^{(r+1)})^{t} \Big]
\end{equation}
**** Remarks
- The space of the orthogonal matrix $D_{k}$ is not convex \to use Flury and
  Gautschi algorithm.
 
- $tr$ \to trace function
**** DONE Questions [1/1]                                       :noexport:
- [X] What is tr?
  Trace \to sum of the diagonal values.
*** DONE M Step - update eigenvalues A
**** Equation
\begin{equation}
\A[km][r+1] = \frac{ \sum_{n=1}^{N} q_{z_{n}}^{(r+1)} (e_{k}) }{ \sum_{n=1}^{N} q_{z_{n}}^{(r+1)} (e_{k}) \tDelta[nkm][r+1] \big[ \D[k][r+1]^{t} (y_{n }- \mu_{k}^{(r+1)}) \big]_{m}^{2} }
\end{equation}
*** DONE M Step - update degree of freedom \gamma, \delta
Solving a system of nonlinear equation:

\begin{cases}
\Psi(\gamma_{km}) = \Psi(\gamma_{km}^{(r+1)}) + \frac{\sum_{n=1}^{N} q_{z_{n}}(e_{k})\ln(\frac{\delta_{km}}
                                                {\tdelta[nkm][r+1]})}
                         {\sum_{l=1}^{N} q_{z_{l}}(e_{k})}\\
\delta_{km} = 1
\end{cases}
**** Remarks
- They are tilde somewhere but I am not sure where...
- The second line is a constant, but I don't see why...
- Still not sure how to solve the systems...

*** TODO M Step - update fields parameters \alpha, \beta
- This step is skipped for now, the optimization of \alpha and \beta is tricky to
  do, and $SpaceM^{3}$ as already something to do this.

- For now $\alpha_{z_{n}}$ can be set to $w_{n}$.
**** DONE Questions [1/1]
- [X] But what about \beta?
  Can be set to any positive value and increased at each iteration.

* TODO Pseudo-code [0/2]
 
Let assume a column major structure of arrays.
# Using ruby syntax just for syntax highlighting
** TODO Variable declarations [0/1]
- [ ] Find the best suited structure/indexing for each arrays
#+BEGIN_EXAMPLE
  int step
  boolean converged
  real thresold

  # Problem size
  int X, Y, Z # image size
  int nb_clusters # will change during EM iteration
  int obs_dimension
  real obs[obs_dimension][X][Y][Z] # observations vectors should be an effective implementation to take advantage of vector operation
  boolean labels[nb_clusters][X][Y][Z]
  int neighbourhood_size # Assuming a cubic neighbourhood

  # Random variables
  real q_z[X][Y][Z][nb_clusters]

  # Model parameters
  real mean[obs_dimension][nb_clusters]
  real eigen_vectors[obs_dimension][obs_dimension][nb_clusters] # D
  real eigen_values[obs_dimension][obs_dimension][nb_clusters] # diagonal matrix A
  real delta[nb_clusters][obs_dimension]
  real gamma[nb_clusters][obs_dimension]
  real alpha[nb_clusters]
  real beta

  # Parameters of the posterior
  real freedom_degree_intensity_posterior[obs_dimension][X][Y][Z][nb_clusters]
  real gamma_p[nb_clusters][obs_dimension]
  real alpha_p[X][Y][Z][nb_clusters]

  real a[obs_dimension] # The orthogonal matrix A as a vector
#+END_EXAMPLE

** TODO Algorithms [0/1]
- [ ] Characteristics of the kernels
#+BEGIN_EXAMPLE
  converged = false

  # Initialization
  while not converged
    ## E-step
    # Compute degree of freedom intensity posterior: tilde delta
  
    # Compute degree of freedom shape posterior: tilde gamma
  
    # Compute external field posterior: tilde alpha

    # update q(zn)
    # update q(wn|zn)
    # update q(wn,zn)
    # update mean
    
    ## M-step
    # update mean
    # upate eigenvectors
    # update eigenvalues
    # update degree of freedom
    # update parameters fields

    if new_likelihood - likelihood < thresold 
      converged = true
    end
    step += 1
  end
#+END_EXAMPLE

*** Posterior parameters
**** Degree of freedom shape
The order of the loops needs to match the structure of the data in
order to minimize the memory accesses. So the most inner loop
correspond the most right index in column major order and the opposite
in a row major order.
#+BEGIN_SRC ruby
  for m in [0..obs_dimension-1]
    for k in [0..nb_clusters-1]
      gamma_p[k][m] = gamma[k][m] + 1/2
    end
  end
#+END_SRC
***** Kernel Characteristics
- m \times k loads
- m \times k adds
- m \times k stores
Twice more memory access than arithmetical operations.
Should be bandwidth bound (at least on CPU).
**** Degree of freedom intensity
***** DONE Naive algo
#+BEGIN_SRC ruby
  real temp[obs_dimension][obs_dimension]
  for k in [0..nb_clusters-1]
    for x in [0..X-1]
      for y in [0..Y-1]
        for z in [0..Z-1]
          temp = eigen_value[k] * transpose(eigen_vector[k]) * (obs[x][y][z] - mean[k]) * transpose(obs[x][y][z] - mean[k]) * eigen_vector[k]
          for m in [0..obs_dimension-1]
            freedom_degree_intensity_posterior[m][k][x][y][z] = delta[k][m] + 1/2 * temp[m][m]
          end
        end
      end
    end
  end
#+END_SRC
****** DONE Kernel characteristics [3/3]

| a: add     |
| p: product |
| l: load    |
| s: store   | 
  
- matrix multiply = $m^{3}(p + a + 3l + s)$

- [X] Complexity: /kxyz/ \times temp + /kxyzm/ \times
  freedom_degree_intensity_posterior = $kxyz(2m^{3}(3l + p + a + s) + m^{2}(7l +
  2p + 2a + 3s) + m(7l + 4s + 2a + p))$ 

- [X] temp: 2m^{3}(3l + p + a + s) + m^{2}(7l + 2p + 2a + 3s) + m(5l + 3s + a)
  - 1 matrix multiply 
  - + 1 matrix vector multiply \to $m²(3l + p + a + s)$
  - + 1 vector transposed vector multiply \to $m²(3l + p + a + s)$
  - + 1 matrix multiply
  - + 2 vector adds \to  /m(2l + s + a)/
  - + 1 matrix transpose \to /m²(l + s)/
  - + 1 vector transpose \to $m(l + s)$
  
- [X] freedom_degree_intensity_posterior: s + 2l + a + p

***** V2
- In the naive algo, some terms used to compute =temp= only depends on
  =k= and thus can be reuse over iterations of inner loops. These terms
  can be computed /k/ times instead of /k \times x \times y \times z \times m/
  times. However we are limited by the non-commutativity of matrix
  product and we might not be able to group all =k= dependent terms
  together to put them in the most external loop.
- The right-hand part can also be computed at once for all =m= of a
  given =k= and [x,y,z]. Thus for a given voxel and cluster this part
  can be computed for all the MRI parameter dimensions stored in
  temporary variable and used to compute the
  =freedom_degree_intensity_posterior=. 
- =obs[x][y][z] - mean[k]= is a simple vector addition, if obs is a
  vector of k's, vectorization by the compiler is possible (if K known at
  compile time).
#+BEGIN_SRC ruby
  real temp1[obs_dimension][obs_dimension]
  real temp2[obs_dimension][obs_dimension]
  real error[obs_dimension] 
  for k in [0..nb_clusters-1]
    temp1 = eigen_value[k] * transpose(eigen_vector[k])
    for x in [0..X-1]
      for y in [0..Y-1]
        for z in [0..Z-1]
          error = obs[x][y][z] - mean[k]
          temp2 = temp1 * (error) * transpose(error) * eigen_vector[k]
          for m in [0..obs_dimension-1]
            freedom_degree_intensity_posterior[m][k][x][y][z] = delta[k][m] + 1/2 * temp2[m][m]
          end
        end
      end
    end
  end
#+END_SRC
****** TODO Kernel characteristics [5/6]
- [X] Complexity: k \times (temp1 + xyz \times (temp2 + error) + xyzm \times
  freedom_degree_intensity_posterior) = $k2m^{3}(p + a + 3l + s) kxyzm^{2}(7l +
  2p + 2a + 3s) + kxyzm^{2}(3l + 2s + a)$

- [X] temp1: $m^{3}(p + a + 3l + s) + m^{2}(l + s)$
  - 1 matrix multiply
  - 1 matrix transpose

- [X] =error=: $m(2l + s + a)$
  - 1 vector add 
    
- [X] temp2: $m^{3}(p + a + 3l + s) + 2m²(3l + p + a + s) + m(l + s)$
  - 1 matrix vector multiply
  - 1 vector transposed vector multiply
  - 1 vector transpose
  - 1 matrix multiply

- [X] freedom_degree_intensity_posterior: s + 2l + a + p
- [ ] How much do we save by putting in the most outer loop?
**** External field
***** Naive algo
#+BEGIN_SRC ruby
  for x in [0..X-1]
    for y in [0..Y-1]
      for z in [0..Z-1]
        for k in [0..nb_clusters-1]
          acc = 0
          for m in [0..obs_dimension-1]
            acc += ln(a[k][m]) + gamma[k][m] * ln(delta[k][m]) - ln(gamma(gamma[k][m])) - gamma_p[k][m] * ln(delta_p[k][x][y][z][m]) + ln(gamma(gamma_p[k][m]))
          end
          alpha_p[x][y][z][k] = transpose(labels[x][y][z]) * alpha + acc
        end
      end
    end
  end
#+END_SRC

****** TODO Kernel characteristics [2/3]
- [ ] Complexity: $xyzk(m \times acc + alpha_p) = xyzk(a + s + l) + xyzkm(5ln + 2gamma + 6a + 3p + 3s + 12l)$
- [X] =acc=: $5ln + 2gamma + 5a + 2p + s + 8l$
  - 5 ln
  - 2 gamma
  - 5 adds
  - 2 mults
  - 1 store
  - 8 loads
- [X] =alpha_p=: $m(4l + 2s + a + p) + a + s + l$
  - 1 transposed vector vector multiply \to $m(3l + s + a + p)$
  - 1 vector transpose
  - 1 add
  - 1 store
  - 1 load
***** TODO V2 [0/1]
In the first algo, we see when computing acc that there are some terms
that depend only on =k= and =m=. That is, part of the computation can be k
x m times instead of  k x n x m by computing these term in a loop on =k=
and =m=. =n= being big, the gain should be non-negligible.  
#+BEGIN_SRC ruby
  for k in [0..nb_clusters-1]
    acc1 = 0
    for m in [0..obs_dimension-1]
      acc1 += ln(a[k][m]) + gamma[k][m] * ln(delta[k][m]) - ln(gamma(gamma[k][m]))  + ln(gamma(gamma_p[k][m]))
    end 
    for x in [0..X-1]
      for y in [0..Y-1]
        for z in [0..Z-1]
          
          acc2 = 0
          for m in [0..obs_dimension-1]
            acc2 += gamma_p[k][m] * ln(delta_p[k][x][y][z][m])
          end
          alpha_p[x][y][z][k] = transpose(labels[x][y][z]) * alpha + acc1 - acc2
        end
      end
    end
  end
#+END_SRC

- By put the loop on the clusters, we are able to 

- [ ] Is the compiler able to find a better transformation?

- [ ] In the /km/ loop there 4 calls to ln maybe 1 should be better, but not
  sure the gain will be significant.

****** TODO Kernel characteristics [3/5]
- [ ] Complexity: $k \times (m \times acc1 + xyzm \times acc2 + xyz \times alpha_p) =
  km(4ln + 2gamma + 6l + s + p + 4a) + kxyzm(ln + 2p + 2a + 7l + 3s) +
  kxyz(2a + 2l + s)$
  The cost of ln and gamma is not known for now, maybe it
  worth it to take look at it...
- [X] =acc1=: $4ln + 2gamma + 6l + s + p + 4a$
  - 4 ln
  - 2 gamma
  - 6 loads
  - 1 store
  - 3 adds
  - 1 mult
- [X] =acc2=: $ln + p + a + 3l + s$
  - 1 ln
  - 3 loads
  - 1 add
  - 1 mult
  - 1 store
- [X] =alpha_p=: $m(4l + 2s + a + p) + 2a + 2l + s$
  - 1 store
  - transpose
  - transposed vector vector multiply
  - 2 adds
  - 2 loads

- [ ] How much do we save by putting k in the most outer loop?
*** TODO q_z [0/4]
- [ ] Add Borders
**** Wrong approach?
#+BEGIN_SRC ruby
  for x in [0..X-1]
    for y in [0..Y-1]
      for z in [0..Z-1]
        for x1 in [x - neighbourhood_size..x - neighbourhood_size]
          for y1 in [y - neighbourhood_size..y - neighbourhood_size]
            for z1 in [z - neighbourhood_size..z - neighbourhood_size]
              if (x1 > 0 and x1 < X) and (y1 > 0 and y1 < Y) and (z1 > 0 and z1 < Z)
                  # do stuff
              end
            end
          end
        end
      end
    end
  end
#+END_SRC
The =if= clause in the loop nest prevents compilers to perform loop
optimization because of the complex control flow is too complex for
them. And additional performance-wise, conditions are costly. Thus we
want to avoid this.

Is it really the wrong approach?

**** TODO Top border
**** TODO Bottom border
**** TODO Center
=neighourhood(x,y,z)= is a function that return voxels that
belongs to the neighbourhood of the voxel at coordinates =[x,y,z]=.
#+BEGIN_SRC ruby
  for x in [neighbourhood_size..X-1-neighbourhood_size]
    for y in [neighbourhood_size..Y-1-neighbourhood_size]
      for z in [neighbourhood_size..Z-1-neighbourhood_size]
        for k in [0..nb_clusters-1]
          neighbours1 = 0
          
          for n in neighourhood(x,y,z) 
            neighbors1 += n[k]
          end

          for k1 in nb_clusters
            W = 0
            neighbours2 = 0
            for n in neighourhood(x,y,z) 
              neighbours2 += n[k1]
            end 
            W += exp(alpha_p[x][y][z][k1] + beta / 2 * neighbours2)
          end
          
          q_z[x][y][z][k] = exp(alpha_p[x][y][z][k] + beta / 2 * neighbours1) / W
        end
      end
    end
  end
#+END_SRC
There are spatial dependencies between voxels but there are no
dependencies between K's. K's can be easily parallelized and could be
vectorized if k's are contiguous in memory. However if the number of
clusters is not known at compile time vectorization will not be done by
the compiler. 

**** TODO Kernel characteristics [0/5]
- [ ] Complexity: $xyzkn \times neighbours1 + xyzk^{2}(n \times neighbours2 + W) +
  xyzk \times q_z$
- [ ] =neighbours1=:
- [ ] =neighbours2=:
- [ ] W:
- [ ] q_z:
* TODO Implementation details
** TODO Manipulated data structures
** TODO Used Libraries
*** TODO BOAST
*** TODO StarPU
** TODO Default Implementation
** TODO Verification / correctness
*** correctness
- What is the delta?
** TODO Optimization
*** TODO Memory access pattern
*** DONE Parallelism - Task paradigm
**** Problem
- *Heterogeneity* of the machines \to Mixing paradigm \to OpenMP,
  CUDA/OpenCL, MPI, etc...
- *Greater number of cores* \to useless if number of
  cores x2 but half stay idle...
- Facing *load balancing* problems

**** Task Graph
- Computation = task \to vertexes
- Dependencies between computation = edges
- A task cannot start if data not arrived from dependencies
- As Z is an HMRF it as non-trivial dependencies \to modeled as a
  task graph 
- We could use StarPU \to handle load balancing & 1 paradigm to target
  CPU, GPGPU and clusters.
*** TODO Code generation approach and tuning
**** Maybe BOAST?
- Meta-programming framework: ruby \to C, Fortran, CUDA, OpenCL
- Generate multiple variant of same code e.g. loop unrolled,
  vectorized, size of tiling, etc...
- *Portable optimization*
- *Code factorization*
- Easier to evaluate performances...
- ... but does it mix well with StarPU and other libraries like Armadillo
*** BOAST example
#+BEGIN_SRC ruby
require 'narray'
require 'BOAST'
include BOAST

set_array_start(0)
set_default_real_size(4)

def vector_add
  n = Int("n",:dir => :in)
  a = Real("a",:dir => :in, :dim => [ Dim(n)] )
  b = Real("b",:dir => :in, :dim => [ Dim(n)] )
  c = Real("c",:dir => :out, :dim => [ Dim(n)] )
  p = Procedure("vector_add", [n,a,b,c]) {
    decl i = Int("i")
    expr = c[i] === a[i] + b[i]
    if (get_lang == CL or get_lang == CUDA) then
      pr i === get_global_id(0)
      pr expr
    else
      pr For(i,0,n-1) {
        pr expr
      }
    end
  }
  return p.ckernel
end
#+END_SRC
*** TODO Performance analysis
** Sandbox                                                         :noexport:
   #+begin_src R :results output :session :exports both
     library("png")
     library("plyr")
     y <- readPNG("images_2.png")
     nrow_img <- nrow(y)
     ncol_img <- ncol(y)
   #+end_src

   #+RESULTS:

   Generate a noisy image:
   #+begin_src R :results output :session :exports both
     # noise_mat <- matrix(rbinom(ncol_img*nrow_img,1,0.2), nrow = nrow_img, ncol = ncol_img)
     # noisy_img <- matrix(bitwXor(y,noise_mat), ncol=ncol_img)

     noise_mat <- matrix(rnorm(ncol_img*nrow_img, mean = 0.5, sd=0.2), nrow = nrow_img, ncol = ncol_img)
     noisy_img <- y + noise_mat
     noisy_img <- (noisy_img - min(noisy_img)) / (max(noisy_img) - min(noisy_img)) # normalization

     image(noisy_img)
   #+end_src

   #+RESULTS:

   #+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 600 :height 400 :session
   hist(noisy_img)
   #+end_src

   #+RESULTS:
   [[file:/tmp/babel-6164uvh/figure61647eQ.png]]


   #+begin_src R :results output :session :exports both

   #+end_src


* Emacs Setup 							   :noexport:
  This document has local variables in its postembule, which should
  allow Org-mode to work seamlessly without any setup. If you're
  uncomfortable using such variables, you can safely ignore them at
  startup. Exporting may require that you copy them in your .emacs.

# Local Variables:
# eval:    (require 'org-install)
# eval:    (org-babel-do-load-languages 'org-babel-load-languages '( (sh . t) (R . t) (perl . t) (ditaa . t) ))
# eval:    (setq org-confirm-babel-evaluate nil)
# eval:    (unless (boundp 'org-latex-classes) (setq org-latex-classes nil))
# eval:    (add-to-list 'org-latex-classes '("memoir" "\\documentclass[smallextended]{memoir} \n \[NO-DEFAULT-PACKAGES]\n \[EXTRA]\n  \\usepackage{graphicx}\n  \\usepackage{hyperref}" ("\\chapter{%s}" . "\\chapter*{%s}") ("\\section{%s}" . "\\section*{%s}") ("\\subsection{%s}" . "\\subsection*{%s}")                       ("\\subsubsection{%s}" . "\\subsubsection*{%s}")                       ("\\paragraph{%s}" . "\\paragraph*{%s}")                       ("\\subparagraph{%s}" . "\\subparagraph*{%s}")))
# eval:    (add-to-list 'org-latex-classes '("acm-proc-article-sp" "\\documentclass{acm_proc_article-sp}\n \[NO-DEFAULT-PACKAGES]\n \[EXTRA]\n"  ("\\section{%s}" . "\\section*{%s}") ("\\subsection{%s}" . "\\subsection*{%s}")                       ("\\subsubsection{%s}" . "\\subsubsection*{%s}")                       ("\\paragraph{%s}" . "\\paragraph*{%s}")                       ("\\subparagraph{%s}" . "\\subparagraph*{%s}")))
# eval:    (setq org-alphabetical-lists t)
# eval:    (setq org-src-fontify-natively t)
# eval:   (setq org-export-babel-evaluate nil)
# eval:   (setq ispell-local-dictionary "english")
# eval:   (eval (flyspell-mode t))
# eval:    (setq org-latex-listings 'minted)
# eval:    (setq org-latex-minted-options '(("bgcolor" "white") ("style" "tango") ("numbers" "left") ("numbersep" "5pt")))
# End:
